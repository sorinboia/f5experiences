[
{
	"uri": "/",
	"title": "Nginx Experience",
	"tags": [],
	"description": "",
	"content": "Welcome to AWS NGINX Meetup – From Application code to Customer Delivering modern applications has upped the needs to provide agile application services free from infrastructure constrains, while ensuring applications run faster, secured and at scale. Choosing the right architecture for application services allows organizations to innovate faster and role new services on any infrastructure or cloud architecture. From code development to customer consumption, NGINX deployed on AWS is helping millions of organizations deliver application infrastructure and services ensuring their application are fast, secure and run at scale.\nIntro to the workshop In this workshop you will experience how to deploy your application in an agile way, using NGINX utilizing AWS compute, applying all the application services needed, such as service mesh, API management and web application firewall in a fully automated way.\nDeploy NGINX infrastructure using Terraform Start by using automation and use Infrastructure as Code concepts to deploy the environment.\nIncrease availability, security and application performance with Kubernetes Nginx Ingress  Monitor the application performance, discover issues and improve performance Authenticate users with OpenID Connect and offload authentication processes from the application to Nginx Zero Trust - Applying Mutual TLS authentication with NGINX  Service Mesh  Trace your application functions within the mesh Secure the interservice communication  Security Secure you application with Nginx App Protect Web Application Firewall.\n Protect you application from fraudulent interactions and separate legitimate traffic from unwanted one Block unwanted Bot traffic to your application  The Nginx Controller  Publish the application APIs with NGINX Micro Gateway Deploy and publish your API endpoints with NGINX Micro Gateway within your Kubernetes environment Enhance APIs with JWT or Access Key token authentication Offload API authentication processes to your API gateway and concentrate Apply API rate limit and spike arrest Protect your APIs from abuse and enforce monetization controls Monitor your application performance Discover your application performance, understand and react when application SLAs are impacted  "
},
{
	"uri": "/070_controller/010_accessing_controller/",
	"title": "Accessing the Nginx Controller",
	"tags": [],
	"description": "",
	"content": " The Nginx Controller has already been deployed with the terraform declaration, we need to find the public IP address.  cd terraform\rexport controller_ip=$(terraform state show \u0026quot;aws_instance.controller\u0026quot; | grep \u0026quot;public_ip\u0026quot; | grep -v \u0026quot;associate_public_ip_address\u0026quot; | cut -d'\u0026quot;' -f2)\rcurl -k -c cookie.txt -X POST --url \u0026quot;https://$controller_ip/api/v1/platform/login\u0026quot; --header 'Content-Type: application/json' --data '{\u0026quot;credentials\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;BASIC\u0026quot;,\u0026quot;username\u0026quot;: \u0026quot;admin@nginx.com\u0026quot;,\u0026quot;password\u0026quot;: \u0026quot;Admin2021\u0026quot;}}'\rexport controller_apikey=$(curl -k -sb cookie.txt -c cookie.txt https://$controller_ip/api/v1/platform/global | jq .currentStatus.agentSettings.apiKey | tr -d '\u0026quot;')\rterraform state show \u0026quot;aws_instance.controller\u0026quot; | grep \u0026quot;public_ip\u0026quot; | grep -v \u0026quot;associate_public_ip_address\u0026quot; | cut -d'\u0026quot;' -f2\rcd ..\rBrowse (using HTTPS) to the IP address of the Controller and verify you have access:  Username (email): admin@nginx.com\nPassword: Admin2021\n\r"
},
{
	"uri": "/030_application/010_app_deployment_1/",
	"title": "App deployment",
	"tags": [],
	"description": "",
	"content": " Deploy the app  kubectl apply -f files/4ingress/1arcadia_delpoy.yaml\rOutput\ndeployment.apps/arcadia-db created\rservice/arcadia-db created\rdeployment.apps/arcadia-frontend created\rservice/arcadia-frontend created\rdeployment.apps/arcadia-login created\rservice/arcadia-login created\rdeployment.apps/arcadia-stock-transaction created\rservice/arcadia-stock-transaction created\rdeployment.apps/arcadia-stocks created\rservice/arcadia-stocks created\rdeployment.apps/arcadia-users created\rservice/arcadia-users created\r Check that all is deployed and working as expected:  kubectl get pods\rOutput\nNAME READY STATUS RESTARTS AGE\rarcadia-db-696f979799-sdzb6 1/1 Running 0 36s\rarcadia-frontend-57ff4f888-v4ggh 1/1 Running 0 36s\rarcadia-login-c995f8dcc-7gvvp 1/1 Running 0 35s\rarcadia-stock-transaction-b996cddb9-xzckf 1/1 Running 0 34s\rarcadia-stocks-dd58548f7-k7tg7 1/1 Running 0 34s\rarcadia-users-768dddd844-8s75q 1/1 Running 0 33s\r kubectl get svc -owide\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR\rarcadia-db ClusterIP 172.20.180.18 \u0026lt;none\u0026gt; 27017/TCP 74s app=arcadia-db\rarcadia-frontend ClusterIP 172.20.11.202 \u0026lt;none\u0026gt; 80/TCP 73s app=arcadia-frontend\rarcadia-login ClusterIP 172.20.247.63 \u0026lt;none\u0026gt; 80/TCP 73s app=arcadia-login\rarcadia-stock-transaction ClusterIP 172.20.155.246 \u0026lt;none\u0026gt; 80/TCP 72s app=arcadia-stock-transaction\rarcadia-stocks ClusterIP 172.20.23.237 \u0026lt;none\u0026gt; 80/TCP 71s app=arcadia-stocks\rarcadia-users ClusterIP 172.20.73.208 \u0026lt;none\u0026gt; 80/TCP 71s app=arcadia-users\rkubernetes ClusterIP 172.20.0.1 \u0026lt;none\u0026gt; 443/TCP 7m2s \u0026lt;none\u0026gt;  The application is not accessible yet. We will deploy the NGINX Ingress in the following sections.\n\r"
},
{
	"uri": "/060_security/010_elk_deployment/",
	"title": "ELK deployment",
	"tags": [],
	"description": "",
	"content": " Deploy ELK in order to be able to visualize and analyze the traffic going through the Nginx App Protect web application firewall  kubectl apply -f files/6waf/elk.yaml\rIn order to connect to our ELK pod, we will need to find the public address of this service:  kubectl get svc elk-web\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\relk-web LoadBalancer 172.20.179.34 a28bd2d8c94214ae0b512274daa06211-2103709514.eu-central-1.elb.amazonaws.com 5601:32471/TCP,9200:32589/TCP,5044:31876/TCP 16h\r 3. Verify that ELK is up and running by browsing to: http://[ELK-EXTERNAL-IP]:5601/.\nPlease note that it might take some time for the DNS name to become available.\n\r"
},
{
	"uri": "/040_ingress/010_ingress_install_1/",
	"title": "Nginx Kubernetes Ingress Installation",
	"tags": [],
	"description": "",
	"content": "We are going to use the Nginx installation manifests based on the Nginx Ingress Controller installation guide. For simplicity - we have already prepared an installation script.\n Run the command bellow:  ./files/4ingress/ingress_install.sh\rOutput\nStarting Nginx Ingress Install\rCloning into \u0026#39;kubernetes-ingress\u0026#39;...\rremote: Enumerating objects: 1270, done.\rremote: Counting objects: 100% (1270/1270), done.\rremote: Compressing objects: 100% (799/799), done.\rremote: Total 34666 (delta 552), reused 1008 (delta 422), pack-reused 33396\rReceiving objects: 100% (34666/34666), 46.90 MiB | 9.88 MiB/s, done.\rResolving deltas: 100% (18747/18747), done.\rUpdating files: 100% (782/782), done.\rUpdating files: 100% (3425/3425), done.\rNote: switching to \u0026#39;v1.10.1\u0026#39;.\rYou are in \u0026#39;detached HEAD\u0026#39; state. You can look around, make experimental\rchanges and commit them, and you can discard any commits you make in this\rstate without impacting any branches by switching back to a branch.\rIf you want to create a new branch to retain commits you create, you may\rdo so (now or later) by using -c with the switch command. Example:\rgit switch -c \u0026lt;new-branch-name\u0026gt;\rOr undo this operation with:\rgit switch -\rTurn off this advice by setting config variable advice.detachedHead to false\rHEAD is now at ba03a73d Release 1.10.1 (#1461)\rnamespace/nginx-ingress created\rserviceaccount/nginx-ingress created\rclusterrole.rbac.authorization.k8s.io/nginx-ingress created\rclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress created\rclusterrole.rbac.authorization.k8s.io/nginx-ingress-app-protect created\rclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-app-protect created\rsecret/default-server-secret created\rconfigmap/nginx-config created\ringressclass.networking.k8s.io/nginx created\rcustomresourcedefinition.apiextensions.k8s.io/virtualservers.k8s.nginx.org created\rcustomresourcedefinition.apiextensions.k8s.io/virtualserverroutes.k8s.nginx.org created\rcustomresourcedefinition.apiextensions.k8s.io/transportservers.k8s.nginx.org created\rcustomresourcedefinition.apiextensions.k8s.io/policies.k8s.nginx.org created\rcustomresourcedefinition.apiextensions.k8s.io/globalconfigurations.k8s.nginx.org created\rglobalconfiguration.k8s.nginx.org/nginx-configuration created\rcustomresourcedefinition.apiextensions.k8s.io/aplogconfs.appprotect.f5.com created\rcustomresourcedefinition.apiextensions.k8s.io/appolicies.appprotect.f5.com created\rcustomresourcedefinition.apiextensions.k8s.io/apusersigs.appprotect.f5.com created\rservice/nginx-ingress created\rdeployment.apps/nginx-ingress created\rconfigmap/nginx-config configured\rInstall finished\r Expose the Nginx Ingress Dashboard.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: v1\rkind: Service\rmetadata:\rname: dashboard-nginx-ingress\rnamespace: nginx-ingress\rannotations:\rservice.beta.kubernetes.io/aws-load-balancer-backend-protocol: \u0026quot;tcp\u0026quot;\rservice.beta.kubernetes.io/aws-load-balancer-type: nlb\rspec:\rtype: LoadBalancer\rports:\r- port: 80\rtargetPort: 8080\rprotocol: TCP\rname: http\rselector:\rapp: nginx-ingress\rEOF\rCheck what we did so far is actually working:  kubectl get svc --namespace=nginx-ingress\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rdashboard-nginx-ingress LoadBalancer 172.20.36.60 aeb592ad4011544219c0bc49581baa13-421891138.eu-central-1.elb.amazonaws.com 80:32044/TCP 11m\rnginx-ingress LoadBalancer 172.20.14.206 ab21b88fec1f445d98c79398abc2cd5d-961716132.eu-central-1.elb.amazonaws.com 80:30284/TCP,443:31110/TCP 5h35m\r Note the EXTERNAL-IP of the \u0026ldquo;dashboard-nginx-ingress\u0026rdquo;. This is the hostname that we are going to use in order to view the Nginx Dashboard.\nBrowse to the following location and verify you can see the dashboard: http://\u0026lt;DASHBOARD-EXTERNAL-IP\u0026gt;/dashboard.html\nNote the EXTERNAL-IP of the \u0026ldquo;nginx-ingress\u0026rdquo;. This is the hostname that we are going to use in order to publish the Arcadia web application.\nBrowse to the following location and verify that you receive a 404 status code: http://\u0026lt;INGRESS-EXTERNAL-IP\u0026gt;/\nPlease note that it might take some time for the DNS names to become available.\n\rSave the EXTERNAL-IPs as env variables for later use  export dashboard_nginx_ingress=$(kubectl get svc dashboard-nginx-ingress --namespace=nginx-ingress | tr -s \u0026quot; \u0026quot; | cut -d' ' -f4 | grep -v \u0026quot;EXTERNAL-IP\u0026quot;)\rexport nginx_ingress=$(kubectl get svc nginx-ingress --namespace=nginx-ingress | tr -s \u0026quot; \u0026quot; | cut -d' ' -f4 | grep -v \u0026quot;EXTERNAL-IP\u0026quot;)\r"
},
{
	"uri": "/080_cleanup/010_cleanup/",
	"title": "Remove configuration",
	"tags": [],
	"description": "",
	"content": " In order to delete the resources created during this workshop, run the commands below:  kubectl delete --all svc --namespace=nginx-mesh\rkubectl delete --all svc --namespace=nginx-ingress\rkubectl delete --all svc --namespace=default\rcd terraform\rterraform destroy --auto-approve\r Finally, delete the previously created Cloud9 stack in the CloudFormation console.  \rPlease note: This will also delete the Cloud9 IDE instance.\n\r"
},
{
	"uri": "/020_terraform/010_terraform_apply_1/",
	"title": "Terraform Apply",
	"tags": [],
	"description": "",
	"content": "  All configuration and management of the infrastructure will be done from the Jumphost.\nSSH into the Jumphost.\n  Watch the terraform deployment status.\n  tail -f startup/startup.log\rWhen the the ALL IS DONE message will appear in the logs you can continue.\n"
},
{
	"uri": "/010_intro/",
	"title": "Intro to the workshop",
	"tags": [],
	"description": "",
	"content": "Intro to the workshop\nThis workshop will provide guidelines on how to deploy an application from scratch in Amazon Elastic Kubernetes Service environment while protecting and enhancing the application availability and usability with Nginx solutions.\nFor this workshop we are going to use the \u0026ldquo;Arcadia Crypto\u0026rdquo; application. The application is built with 6 different microservices that are deployed in the Kubernetes environment.\nBy the end of the workshop the \u0026ldquo;Arcadia Crypto\u0026rdquo; will be fully deployed and protected as described in the bellow diagram.\n"
},
{
	"uri": "/020_terraform/020_eks_1/",
	"title": "AWS EKS",
	"tags": [],
	"description": "",
	"content": "While you wait, you can review the Introduction section of the AWS EKS Workshop to learn about Kubernetes and Amazon EKS basics.\nManaged control plane Amazon EKS provides a scalable and highly-available control plane that runs across multiple AWS availability zones. The Amazon EKS service automatically manages the availability and scalability of the Kubernetes API servers and the etcd persistence layer for each cluster. Amazon EKS runs the Kubernetes control plane across three Availability Zones in order to ensure high availability, and it automatically detects and replaces unhealthy masters.\nManaged worker nodes Amazon EKS lets you create, update, or terminate worker nodes for your cluster with a single command. Managed node groups run nodes using the latest EKS-optimized AMIs in your AWS account while updates and terminations gracefully drain nodes to ensure your applications stay available.\n"
},
{
	"uri": "/020_terraform/",
	"title": "Deploy NGINX infrastructure using Terraform",
	"tags": [],
	"description": "",
	"content": "In UDF a startup script will automatically deploy the EKS cluster and the Nginx Controller.\n"
},
{
	"uri": "/070_controller/020_microgateway/",
	"title": "Microgateway deployment",
	"tags": [],
	"description": "",
	"content": " Deploy the microgateway with the following configuration.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: microgateway\rspec:\rreplicas: 1\rselector:\rmatchLabels:\rapp: microgateway\rtemplate:\rmetadata:\rlabels:\rapp: microgateway\rspec:\rcontainers:\r- name: microgateway\rimage: $AWS_ACCOUNT_ID.dkr.ecr.eu-central-1.amazonaws.com/arcadia-microgateway:v1\rimagePullPolicy: Always\renv:\r- name: ENV_CONTROLLER_API_URL\rvalue: $controller_ip:443\r- name: ENV_CONTROLLER_API_KEY\rvalue: $controller_apikey\rports:\r- containerPort: 80\r- containerPort: 443\r---\rapiVersion: v1\rkind: Service\rmetadata:\rname: microgateway\rspec:\rselector:\rapp: microgateway\rports:\r- port: 80\rtargetPort: 80\rname: http\r- port: 443\rtargetPort: 443\rname: https\rexternalTrafficPolicy: Local\rtype: LoadBalancer\rEOF\rFrom now on we will only use the Controller GUI do to all of our configuration.\nThe end goal will be to expose and protect our APIs both internally within the cluster and externally to other programmers.\nLogin to the Nginx Controller web UI, click the \u0026ldquo;N\u0026rdquo; button in the upper left corner and go to \u0026ldquo;Infrastructure\u0026rdquo; -\u0026gt; \u0026ldquo;Instances\u0026rdquo;.  You will see the microgateway we just deployed listed. If it is not there wait for about 2 minutes, it might take a little bit of time for the instance to register.\nGet the EXTERNAL-IP of the microgateway service we just published, we will use it later within our config.  export microhost=$(kubectl get svc microgateway | tr -s \u0026quot; \u0026quot; | cut -d' ' -f4 | grep -v \u0026quot;EXTERNAL-IP\u0026quot;) \u0026amp;\u0026amp; echo $microhost\rOutput\naa2ba08e2b4024a85ba93aa32d0bafac-603500592.eu-central-1.elb.amazonaws.com\r "
},
{
	"uri": "/060_security/020_nap_deployment/",
	"title": "Nginx App Protect deployment",
	"tags": [],
	"description": "",
	"content": " Prepare the Nginx App Protect config  export elk_log=$(kubectl get svc elk-log | tr -s \u0026quot; \u0026quot; | cut -d' ' -f3 | grep -v \u0026quot;CLUSTER-IP\u0026quot;)\rcat \u0026lt;\u0026lt; EOF | kubectl apply -f -\r# APPolicy is the policy configuration. Here we are enabling signature check of known attacks\rapiVersion: appprotect.f5.com/v1beta1\rkind: APPolicy\rmetadata: name: attacksigs\rspec:\rpolicy:\rname: attacksigs\rtemplate:\rname: POLICY_TEMPLATE_NGINX_BASE\rapplicationLanguage: utf-8\renforcementMode: blocking\rsignature-sets:\r- name: All Signatures\rblock: true\ralarm: true\r---\r# We are going to log to ELK all requests not only the blocked ones\rapiVersion: appprotect.f5.com/v1beta1\rkind: APLogConf\rmetadata:\rname: logconf\rspec:\rcontent:\rformat: default\rmax_message_size: 64k\rmax_request_size: any\rfilter:\rrequest_type: all\r---\r# Creating a generic policy and gluing things together\rapiVersion: k8s.nginx.org/v1\rkind: Policy\rmetadata:\rname: waf-policy\rspec:\rwaf:\renable: true\rapPolicy: \u0026quot;default/attacksigs\u0026quot;\rsecurityLog:\renable: true\rapLogConf: \u0026quot;default/logconf\u0026quot;\rlogDest: \u0026quot;syslog:server=$elk_log:5144\u0026quot;\rEOF\rEnable Nginx App Protect on the VS  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: k8s.nginx.org/v1\rkind: VirtualServer\rmetadata:\rname: arcadia\rspec:\rhost: $nginx_ingress tls:\rsecret: arcadia-wildcard # Represents the server certificate\rredirect:\renable: true # Always redirect to https if incoming request is http\r# The bellow waf policy is attachment config\rpolicies: - name: waf-policy\rupstreams:\r- name: arcadia-users\rservice: arcadia-users\rport: 80\rhealthCheck: # This is the most basic healthcheck config for more info follow this link https://docs.nginx.com/nginx-ingress-controller/configuration/virtualserver-and-virtualserverroute-resources/#upstream-healthcheck\renable: true\rpath: /healthz\r- name: arcadia-login\rservice: arcadia-login\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stocks\rservice: arcadia-stocks\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stock-transaction\rservice: arcadia-stock-transaction\rport: 80\rhealthCheck: enable: true\rpath: /healthz\r- name: arcadia-frontend\rservice: arcadia-frontend\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\rroutes:\r- path: /v1/user location-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-users\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: /v1/login\rlocation-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; action:\rpass: arcadia-login\r- path: /v1/stock\rlocation-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; action:\rpass: arcadia-stocks\r- path: /v1/stockt\rlocation-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-stock-transaction\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: / location-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; action:\rpass: arcadia-frontend\rEOF\r"
},
{
	"uri": "/050_service_mesh/020_nsm_deploy/",
	"title": "Nginx Service Mesh deployment",
	"tags": [],
	"description": "",
	"content": " Deploy all Nginx components in the Kubernetes environment  nginx-meshctl deploy --registry-server \u0026quot;$AWS_ACCOUNT_ID.dkr.ecr.eu-central-1.amazonaws.com\u0026quot; --image-tag 0.9.0 --sample-rate 1 --disable-auto-inject\rOutput\nDeploying NGINX Service Mesh Control Plane in namespace \u0026#34;nginx-mesh\u0026#34;...\rCreated namespace \u0026#34;nginx-mesh\u0026#34;.\rW0502 08:20:37.942849 7607 warnings.go:70] apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16\u0026#43;, unavailable in v1.22\u0026#43;; use apiextensions.k8s.io/v1 CustomResourceDefinition\rCreated SpiffeID CRD.\rW0502 08:20:38.076108 7607 warnings.go:70] admissionregistration.k8s.io/v1beta1 ValidatingWebhookConfiguration is deprecated in v1.16\u0026#43;, unavailable in v1.22\u0026#43;; use admissionregistration.k8s.io/v1 ValidatingWebhookConfiguration\rWaiting for SPIRE to be running...done.\rDeployed Spire.\rDeployed NATS server.\rCreated traffic policy CRDs.\rDeployed Mesh API.\rDeployed Metrics API Server.\rDeployed Prometheus Server nginx-mesh/prometheus.\rDeployed Grafana nginx-mesh/grafana.\rDeployed tracing server nginx-mesh/zipkin.\rAll resources created. Testing the connection to the Service Mesh API Server...\rConnected to the NGINX Service Mesh API successfully.\rVerifying that all images were pulled...done.\rNGINX Service Mesh is running.\r All components have been deployed in the \u0026ldquo;nginx-mesh\u0026rdquo; namespace, run the following command and validate that all pods are running  kubectl get pods -n nginx-mesh\rOutput\ngrafana-766d495d65-mjngv 1/1 Running 0 5m30s\rnats-server-7d457c74d9-csjt2 1/1 Running 0 5m34s\rnginx-mesh-api-575858b96b-djq7h 1/1 Running 0 5m32s\rnginx-mesh-metrics-57dd4796b8-6mtqc 1/1 Running 0 5m31s\rprometheus-67dc46b8b6-2wkgg 1/1 Running 0 5m30s\rspire-agent-nv5z2 1/1 Running 0 6m18s\rspire-server-0 2/2 Running 0 6m18s\rzipkin-564b9d4954-4hqdr 1/1 Running 0 5m29s\r "
},
{
	"uri": "/040_ingress/020_ingress_01/",
	"title": "Publish the app",
	"tags": [],
	"description": "",
	"content": "Expose all the application services and route traffic based on the HTTP path. We will start with a basic configuration.\n Expose Arcadia to the world.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: k8s.nginx.org/v1\rkind: VirtualServer\rmetadata:\rname: arcadia\rspec:\rhost: $nginx_ingress upstreams:\r- name: arcadia-users\rservice: arcadia-users\rport: 80\r- name: arcadia-login\rservice: arcadia-login\rport: 80\r- name: arcadia-stocks\rservice: arcadia-stocks\rport: 80\r- name: arcadia-stock-transaction\rservice: arcadia-stock-transaction\rport: 80\r- name: arcadia-frontend\rservice: arcadia-frontend\rport: 80\rroutes:\r- path: /v1/user action:\rpass: arcadia-users\r- path: /v1/login action:\rpass: arcadia-login\r- path: /v1/stock action:\rpass: arcadia-stocks\r- path: /v1/stockt action:\rpass: arcadia-stock-transaction\r- path: / action:\rpass: arcadia-frontend\rEOF\rNote how the various HTTP paths (/v1/user, /v1/login, /v1/stockt) are routed by Ingress to the relevant K8s services.\nAt this stage the basic install is finished and all that\u0026rsquo;s left is to check the connectivity to the Arcadia web application. Get the public hostname of the exposed nginx-ingress service.\n Browse to the following location and verify that you can access the site: http://\u0026lt;INGRESS-EXTERNAL-IP\u0026gt;/\n  Login to the application using the following credentials:\n  Username: satoshi@bitcoin.com\nPassword: bitcoin\n\rAt the moment we still have two key features missing:\n We are serving only http, not https. We want our site to be fully secured therefore all communications need to be encrypted We are not actively monitoring the health of the pods through the data path   Take a look at the files/4ingress/1arcadia_increase.yaml file. It increases the number of pods that our services use.\n  Apply this new configuration.\n  kubectl apply -f files/4ingress/1arcadia_increase.yaml\rLook at the Nginx dashboard and click on \u0026ldquo;HTTP Upstreams\u0026rdquo;, you can see that right now all services have two members but no health check.  "
},
{
	"uri": "/070_controller/030_configuration/",
	"title": "Build the configuration",
	"tags": [],
	"description": "",
	"content": " Create an environment  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Environments\u0026rdquo; -\u0026gt; \u0026ldquo;Create Environment\u0026rdquo; In all the fields, enter the following value: prod.\nClick on \u0026ldquo;View API Request\u0026rdquo;.\nAll the configuration on the Nginx Controller can be easlly automated with external orchestration systems, this view can help you in understanding how to generate the configuration API calls.\nThe output will look like this: Output\n{\r\u0026#34;metadata\u0026#34;: {\r\u0026#34;name\u0026#34;: \u0026#34;prod\u0026#34;,\r\u0026#34;displayName\u0026#34;: \u0026#34;prod\u0026#34;,\r\u0026#34;description\u0026#34;: \u0026#34;prod\u0026#34;,\r\u0026#34;tags\u0026#34;: [\r\u0026#34;prod\u0026#34;\r]\r},\r\u0026#34;desiredState\u0026#34;: {}\r}\r\nClick \u0026ldquo;Submit\u0026rdquo;.\nCreate the Certificate:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Certs\u0026rdquo; -\u0026gt; \u0026ldquo;Create Cert\u0026rdquo;  Name: server-cert\nEnvironment: prod \nChose \u0026ldquo;Copy and paste PEM text\u0026rdquo;\nPrivate Key: Browse to https://raw.githubusercontent.com/sorinboia/nginx-experience-aws-ac2.0/main/files/7controller/ca.key copy and paste.\nPublic Cert: Browse to https://raw.githubusercontent.com/sorinboia/nginx-experience-aws-ac2.0/main/files/7controller/ca.pem copy and paste.\nSubmit\n Create the Gateway:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Gateways\u0026rdquo; -\u0026gt; \u0026ldquo;Create Gateway\u0026rdquo;  Name: api.arcadia.aws.cloud\nEnvironment: prod\nInstance Refs: Select All\nHostname: https://\u0026lt;EXTERNAL-IP OF THE \u0026quot;microgateway\u0026quot; SERVICE\u0026gt;\nCert Reference: server-cert\nSubmit\n Create the App:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;Apps\u0026rdquo; -\u0026gt; \u0026ldquo;Create App\u0026rdquo;  Name: arcadia-api\nEnvironment: prod\nSubmit\n So far we have created an environment, uploaded the certificate/key that we will use for our HTTPS connection, created a gateway which represents our entry point into the API gateway and last defined a new application object.\n"
},
{
	"uri": "/030_application/",
	"title": "Deploy The Arcadia Crypto application",
	"tags": [],
	"description": "",
	"content": "We will deploy our application in the Kubernetes environment.\nAs stated before these are the 6 microservices which we will deploy.\n Frontend - serves the non dynamic content for like html, js, css and images Login - in in charge of dealing with anything related to the login user functionality Users - all user data interaction is done through this microservice only Stocks - connects to external resources to get the latest crypto data and serves it to the application clients Stocks Transaction - Deal with all related to buying or selling crypto currencies. It interact with other microservices like Users and Stocks Database - Database were all information is stored  "
},
{
	"uri": "/020_terraform/030_eks_verification_1/",
	"title": "EKS Verification",
	"tags": [],
	"description": "",
	"content": " Check and see that our cluster is up an running.\nBelow we should see our two K8s worker nodes:  kubectl get nodes\rOutput\nNAME STATUS ROLES AGE VERSION\rip-10-0-3-204.eu-central-1.compute.internal Ready \u0026lt;none\u0026gt; 100s v1.19.6-eks-49a6c0  Go to the lab directory  cd lab\r"
},
{
	"uri": "/040_ingress/030_health_https_1/",
	"title": "Enable https and monitoring",
	"tags": [],
	"description": "",
	"content": "In our next step we will finish this part of the configuration, we will implement the following:\n Enable health checks Enable https for the application and redirect http requests to https   Apply the configuration.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: v1\rdata:\rtls.crt: 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\rtls.key: LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0tCk1JSUV2UUlCQURBTkJna3Foa2lHOXcwQkFRRUZBQVNDQktjd2dnU2pBZ0VBQW9JQkFRRE1LQkNYWHdaL0xQbzMKZjNqTEdyL2poTUVzU2Zna09xKzhrTlBKcVc3T2xrQms0a2VrZWhLRVVVQ0MwUHdUazgrbnp0Skg3bzBEOUNaaApGblVvaUhlZ2ZEc01ERTVhaFV4eG5UNXM5Zlh2b0ZSakp5WFFEdFZYb0dVUE1ubzJlVkdYYi81Qm1LeVNJMDVpCkNweUgrTVQvWERxYXdwYVdmbEwyeTl6WStVdjRpZEFYbGRaQ1JwVUhITldXZjhPeUMvcFJiZndhYVYzRDJ6RjEKWnhyemdiSHFnd1l6ZERzZ2F1ZHZOejdKaHMrQ2hDcS9SUDVoTEhjZDc1eW01bmppTFZsVmcvYUZJS09IT2wyZwpURXZ0OFlnY013VEloMDY0a3FqVUt4OVAwZkJKWUszcFdGaHpYcGhDR2lndTdTd3EyYnpGTkZabjY1SEdtSDBJCmNUSzFZTUpuQWdNQkFBRUNnZ0VCQUxlRm1JeUtWb3ZDUWRmYjFoazJIYU1IOXFLNmg3OEJwbkpxK29lQXNPUXkKdXdZMVIxTzZqS1MzYWJkdTUvc3RWM0V4QVdTNk03ZUhKVzBIUkNCQXRORG1mQ256Y2dmN1plR0lDZExuTThMSwpMTEhKUWp1SzZndFFXeWhUSnZ1TENXYUp2VlQ0U3NFL3pibGljcDNrYWlwbm5TTDdvMlQ3ZjlidEljVkdMUjNvCklhNU1SWnZ3ZDRHZzBkakErNUIwNWhXU3lDT3k0OHJFVzVKMUdCdC90eTBlaHhEOVVUVS9nb0ViL1Q1TXY1OVYKMWY2elNjS0lHbTh3emx5UnVZbWZwTkwrYjFkdFVUb1NUdVA5R0loU3FWTFN1NUJpR2RTMk81MFhqOWVUQlFhRgpRZUtrN002dWdjanZtZHdzTW4vdHM1T2pvVjY0bk10QlNWemY1VWhJdm1rQ2dZRUE2aGQwZWFHK3J1TFhKTDQwCncreUVuUDZtTktYQkI3eVNTWkVtck1aOWxGVUFxOVdQUk9pZnQ5Tm1vc1BGWi9QMDNBcWFhb3cxTGJmdjBMMFQKQ0hyNkJZSGhtYkNVTjJ6TExrRjZSVnI4NUhtQUxjdVhyajVEM3pZd3lZVmVJOThzV0RZcGVsaWtnOUFnNUFtagpNc3ZBTnRYRElJQmlXTkFQQzJoWXhORisrTHNDZ1lFQTMwTm15ZDFzSi9UaFJvUFpoK2dMNmZ0S2RySE1EWk1zCjRHZkI5K2FDTDNtR3NwU094MTlHOHJmbG9ZU2xNTE5HdnlZcGo1SGg1aHJnQXc4aGVBS3BFRmxFVTdBcm54dVcKZmwwc2J3RkhwN05pZUZscGFPdzU2Mm9FN3c1M2pPRHBIQ1ZLSFphQ0lHZjVLSS9tQmRuVWZYZ1JzSzExdjZMTwpJaExYMFNSM3FFVUNnWUFscUpaTlJ1NzFGWHNFNXpCMzRHSEpHOUpESC9NNHVtWlNQVzZhVnVnMjU2SFBBdkVrClpjUGovN2RBTWZ4YzU4c1padjlHYXIzWFdBTFZjc1ZRRlBDSjJFWWh3bDFsdVRQS2dqQVlYalhXejVFR1RQMWUKdzVlSm5oOGxIRFp6ME9CQ1pKd3htWGNGMllLaERNZmJVUm5mK0cyR21nQzRSdWhVcm5teFYvNTBKd0tCZ0RmSAo2VWlLTDltVHp5MEZDRFp5ZlhlS04wS01qWVRldnBtYWt0WFRHN2VzejBDUzZWRmF0cWt4MVFlVDBvbm1ZTWlsClNrRDZtOHdYN3R3VXpiSGtTRVV6YUdUWVlTMnhnTm8xZ1VLQ3VWcG04VFZNY1krclpaVXh1ZVhZWVhvclAxS3UKNW1PYUZRenZyVXE3R1NkaEV6djk0YjJZdVJDV0pwWlF5dWNRQzIxWkFvR0FKbnNGeHl4ZmFXajFEVHUxcEttNQpTSHQvL0cyRGZteHhKQ05aTFZMUHJYVDJhWmdrNWQwMW5Zb3dydCtscCtiSXBLc09BS2MxamxtRVNwT2VaTEU1Cmk0UVV0eEs3SnV5dHNNZXdqUCtYdmc3VFNEN3d1bW95Vkh6RS9VOTBYcldPOVJJd1VlZFpmOFg4bjVPNCtaRisKeWpqaWNlcGY1ci9DaFNhZG1jM1RwMFk9Ci0tLS0tRU5EIFBSSVZBVEUgS0VZLS0tLS0K\rkind: Secret\rmetadata:\rname: arcadia-wildcard\rtype: kubernetes.io/tls\r---\rapiVersion: k8s.nginx.org/v1\rkind: VirtualServer\rmetadata:\rname: arcadia\rspec:\rhost: $nginx_ingress tls:\rsecret: arcadia-wildcard # Represents the server certificate\rredirect:\renable: true # Always redirect to https if incoming request is http\rupstreams:\r- name: arcadia-users\rservice: arcadia-users\rport: 80\rhealthCheck: # This is the most basic healthcheck config for more info follow this link https://docs.nginx.com/nginx-ingress-controller/configuration/virtualserver-and-virtualserverroute-resources/#upstream-healthcheck\renable: true\rpath: /healthz\r- name: arcadia-login\rservice: arcadia-login\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stocks\rservice: arcadia-stocks\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stock-transaction\rservice: arcadia-stock-transaction\rport: 80\rhealthCheck: enable: true\rpath: /healthz\r- name: arcadia-frontend\rservice: arcadia-frontend\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\rroutes:\r- path: /v1/user action:\rpass: arcadia-users\r- path: /v1/login action:\rpass: arcadia-login\r- path: /v1/stock action:\rpass: arcadia-stocks\r- path: /v1/stockt action:\rpass: arcadia-stock-transaction\r- path: / action:\rpass: arcadia-frontend\rEOF\rBrowse to the Arcadia website with http and you will be automatically redirected to https.\nLook at the Nginx dashboard and observe that Nginx has started monitoring the pods.  "
},
{
	"uri": "/060_security/030_nap_test/",
	"title": "Nginx App Protect testing",
	"tags": [],
	"description": "",
	"content": "  Browse again to the Arcadia web app and verify that it is still working.\n  Let\u0026rsquo;s simulate a Cross Site Scripting (XSS) attack, and make sure it\u0026rsquo;s blocked:\n  https://\u0026lt;INGRESS-EXTERNAL-IP\u0026gt;/?a=%3Cscript%3Ealert(%27xss%27)%3C/script%3E\nEach of the blocked requests will generate a support ID, save it for later.\nBrowse to the ELK as before and click the \u0026ldquo;Discover\u0026rdquo; button:  Here, you\u0026rsquo;ll see all the request logs, allowed and blocked, sent by the Nginx WAF to ELK.\nLet\u0026rsquo;s look for the reason why our attack requests were blocked.\nAdd a filter with the support ID you have received as seen bellow:  In the right side of the panel, you can see the full request log and the reason why it was blocked.\nContinue and explore the visualization capabilities of Kibana and log information from Nginx WAF by looking into the next two sections bellow the \u0026ldquo;Discover\u0026rdquo; button (Visualize and Dashboard -\u0026gt; Overview).  "
},
{
	"uri": "/050_service_mesh/030_side_car_injection/",
	"title": "Side Car Injection",
	"tags": [],
	"description": "",
	"content": " Inject the sidecar into the application pods  kubectl get deployment arcadia-frontend -oyaml | nginx-meshctl inject | kubectl apply -f -\rkubectl get deployment arcadia-login -oyaml | nginx-meshctl inject | kubectl apply -f -\rkubectl get deployment arcadia-stock-transaction -oyaml | nginx-meshctl inject | kubectl apply -f -\rkubectl get deployment arcadia-stocks -oyaml | nginx-meshctl inject | kubectl apply -f -\rkubectl get deployment arcadia-users -oyaml | nginx-meshctl inject | kubectl apply -f -\rWait for the sidecar to be injected in all pods and verify that the pods have 2 containers  kubectl get pods\rOutput\nNAME READY STATUS RESTARTS AGE\rarcadia-db-6cb9cfcb44-l2v26 1/1 Running 0 85m\rarcadia-frontend-59b7c8d784-4xxvf 2/2 Running 0 6m25s\rarcadia-frontend-59b7c8d784-nzz5b 2/2 Running 0 6m11s\rarcadia-login-8599ff74b7-hg4mc 2/2 Running 0 6m11s\rarcadia-login-8599ff74b7-sf4p4 2/2 Running 0 6m24s\rarcadia-stock-transaction-684f86d649-5mzbf 2/2 Running 1 6m22s\rarcadia-stock-transaction-684f86d649-nqk4s 2/2 Running 1 6m7s\rarcadia-stocks-56ccc548d5-c48mf 2/2 Running 0 6m21s\rarcadia-stocks-56ccc548d5-jzq64 2/2 Running 0 6m11s\rarcadia-users-6df4f9fb7c-42dsr 2/2 Running 1 6m19s\rarcadia-users-6df4f9fb7c-8jv8h 2/2 Running 1 6m8s\r Propagate the opentracing headers so we can get full visibility within the mesh  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: k8s.nginx.org/v1\rkind: VirtualServer\rmetadata:\rname: arcadia\rspec:\rhost: $nginx_ingress tls:\rsecret: arcadia-wildcard # Represents the server certificate\rredirect:\renable: true # Always redirect to https if incoming request is http\rupstreams:\r- name: arcadia-users\rservice: arcadia-users\rport: 80\rhealthCheck: # This is the most basic healthcheck config for more info follow this link https://docs.nginx.com/nginx-ingress-controller/configuration/virtualserver-and-virtualserverroute-resources/#upstream-healthcheck\renable: true\rpath: /healthz\r- name: arcadia-login\rservice: arcadia-login\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stocks\rservice: arcadia-stocks\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stock-transaction\rservice: arcadia-stock-transaction\rport: 80\rhealthCheck: enable: true\rpath: /healthz\r- name: arcadia-frontend\rservice: arcadia-frontend\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\rroutes:\r- path: /v1/user location-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-users\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: /v1/login\rlocation-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; action:\rpass: arcadia-login\r- path: /v1/stock\rlocation-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; action:\rpass: arcadia-stocks\r- path: /v1/stockt\rlocation-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-stock-transaction\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: /\rlocation-snippets: |\ropentracing_propagate_context;\ropentracing_operation_name \u0026quot;nginx-ingress\u0026quot;; action:\rpass: arcadia-frontend\rEOF\r"
},
{
	"uri": "/040_ingress/040_oidc_1/",
	"title": "Authentication with Openid Connect and Okta",
	"tags": [],
	"description": "",
	"content": "Due to unsecure credentials storage of the application it has been decided to migrate to Okta which will provide user management and authentication without the need to store localy personal information.\n Redeploy the front and microservice to a new version in order to use Openid.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: arcadia-frontend\rspec:\rreplicas: 2\rselector:\rmatchLabels:\rapp: arcadia-frontend\rtemplate:\rmetadata:\rlabels:\rapp: arcadia-frontend\rspec:\rcontainers:\r- name: arcadia-frontend\rimage: sorinboiaf5/arcadia-frontend:oktav0.1\rimagePullPolicy: Always\rports:\r- containerPort: 80\rEOF\rNext we need to configure our Ingress controller to validate each request based on the JWT token and if valid add a custom header which indicates to the application the user.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\r# This secret contains the public json web keys needed to validate the token apiVersion: v1\rkind: Secret\rmetadata:\rname: jwk-secret\rtype: nginx.org/jwk\rdata:\rjwk: eyJrZXlzIjpbeyJrdHkiOiJSU0EiLCJhbGciOiJSUzI1NiIsImtpZCI6IlJINk8wYUlBMTNXVENickJjMzBuaV9RTllUdF9OWFRBMWZlcGdyUGdwalEiLCJ1c2UiOiJzaWciLCJlIjoiQVFBQiIsIm4iOiJoOGJyMmZYODRsVExINC1FREhfUWg2NXY5RXIwaUtlQkhST09xTmJ3dzFRYTNaVU9tMU42dE54Q3UtMU54d0x6Y25xOEszcEE1WVlVNkRFcTBhQWF4OEpxNlVhcEFrbVcxYkh3WnhTTnRHcUwyTXVlM3VFNHdnRWF6VlRrMjhqY2FxRmpZRDlVaU12UG9IZnFHUjRsVXhxbEZiNTVDdnJCZ3VnNUFkaXdoNWl4d3NSODZ0TUpFSE84cGxVSmtqd3JFNmYtSVlTNHBxelo5X2RXSGtJcmZ3dEtlZ000VzBqSm1kX3lBc0I3VVVlWGp1dzNRMjhoZWE4UXp5dVZ5YUVzcTJRM01iWjBmdTJ2U3RlMjNkakxiaXR1b2JoTzVCaEhEYkpaZUVFVjlMellsSHRSSEx6ZUV0OFdLTS1oWUNlSXpQSkt4OGtzdlJTRDE2Tk4xY19NZVEifV19\r---\r# Nginx Policy which will be be used in the Virtual Server to perform the JWT validation\rapiVersion: k8s.nginx.org/v1\rkind: Policy\rmetadata:\rname: jwt-policy\rspec:\rjwt:\rrealm: apis\rsecret: jwk-secret\r---\rapiVersion: k8s.nginx.org/v1\rkind: VirtualServer\rmetadata:\rname: arcadia\rspec:\rhost: $nginx_ingress tls:\rsecret: arcadia-wildcard # Represents the server certificate\rredirect:\renable: true # Always redirect to https if incoming request is http\rupstreams:\r- name: arcadia-users\rservice: arcadia-users\rport: 80\rhealthCheck: # This is the most basic healthcheck config for more info follow this link https://docs.nginx.com/nginx-ingress-controller/configuration/virtualserver-and-virtualserverroute-resources/#upstream-healthcheck\renable: true\rpath: /healthz\r- name: arcadia-login\rservice: arcadia-login\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stocks\rservice: arcadia-stocks\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stock-transaction\rservice: arcadia-stock-transaction\rport: 80\rhealthCheck: enable: true\rpath: /healthz\r- name: arcadia-frontend\rservice: arcadia-frontend\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\rroutes:\r- path: /v1/user # These directives attach the JWT policy to the route that needs authentication extract the username/email address and add it as a header policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-users\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: /v1/login action:\rpass: arcadia-login\r- path: /v1/stock action:\rpass: arcadia-stocks\r- path: /v1/stockt policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-stock-transaction\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: / action:\rpass: arcadia-frontend\rEOF\rNext we need to register the application end points to the Openid Connect provider  curl https://okta.vltr.nginx-experience.com/okta-update?domain=$nginx_ingress\rLogout of the Arcadia Crypto application, clear your cache or open the browser in incognito and login with the credentials bellow. The authentication will be done by Okta.  Username: satoshi@bitcoin.com\nPassword: 1qaz!@#$\n\r"
},
{
	"uri": "/060_security/040_bot_protection/",
	"title": "Bot Protection",
	"tags": [],
	"description": "",
	"content": "Our application APIs at the moment are published only for consumption by browsers.\nTherefor we don\u0026rsquo;t want to allow any kind of automated tool to access these endpoints.\nWe are going to block not only malicious bots but also tools like curl.\n Try and access the stock microservice and get data with curl.  curl -k https://$nginx_ingress/v1/stock/ticker/all\rThe request will succeed.\nApply the bot config  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: appprotect.f5.com/v1beta1\rkind: APPolicy\rmetadata: name: attacksigs\rspec:\rpolicy:\rname: attacksigs\rtemplate:\rname: POLICY_TEMPLATE_NGINX_BASE\rapplicationLanguage: utf-8\renforcementMode: blocking\rsignature-sets:\r- name: All Signatures\rblock: true\ralarm: true\r# The bellow config is in charge of defining what bot to block and whom to allow bot-defense:\rsettings:\risEnabled: true\rmitigations:\rclasses:\r- name: trusted-bot\raction: alarm\r- name: untrusted-bot\raction: block\r- name: malicious-bot\raction: block\rEOF\rTry again in a few seconds to access the stock microservice and get data with curl.  curl -k https://$nginx_ingress/v1/stock/ticker/all\rThis time the request has been blocked. Take the Support Id and look for the logs in Kibana\n"
},
{
	"uri": "/070_controller/040_apis/",
	"title": "Import the OpenApi definition",
	"tags": [],
	"description": "",
	"content": "Next we are going to publish the application APIs to the world.\nThere are two ways of creating this configuration, the first one is manual similar to the way we performed the configuration until this point and the second one is described bellow.\nAs part of their development cycle, the developers of the Arcadia application are generating an OpenApi specification to describe their APIs.\nWe are going to use this API specification in order to publish the services to the world.\n Run the following curl commands.  curl -k -sc cookie.txt -X POST --url \u0026quot;https://$controller_ip/api/v1/platform/login\u0026quot; --header 'Content-Type: application/json' --data '{\u0026quot;credentials\u0026quot;: {\u0026quot;type\u0026quot;: \u0026quot;BASIC\u0026quot;,\u0026quot;username\u0026quot;: \u0026quot;admin@nginx.com\u0026quot;,\u0026quot;password\u0026quot;: \u0026quot;Admin2021\u0026quot;}}' curl -k -sb cookie.txt -c cookie.txt --location --request PUT \u0026quot;https://$controller_ip/api/v1/security/identity-providers/apikey\u0026quot; --header 'Content-Type: application/json' --header 'Content-Type: text/plain' --data \u0026quot;@files/7controller/arcadia_idp.json\u0026quot;\rcurl -k -sb cookie.txt -c cookie.txt --location --request PUT \u0026quot;https://$controller_ip/api/v1/security/identity-providers/apikey/clients\u0026quot; --header 'Content-Type: application/json' --header 'Content-Type: text/plain' --data \u0026quot;@files/7controller/arcadia_idp_key.json\u0026quot;\rcurl -k -sb cookie.txt -c cookie.txt --location --request PUT \u0026quot;https://$controller_ip/api/v1/services/api-definitions/arcadia-api/versions/1.0.0\u0026quot; --header 'Content-Type: application/json' --header 'Content-Type: text/plain' --data \u0026quot;@files/7controller/arcadia_api_spec.json\u0026quot;\rcurl -k -sb cookie.txt -c cookie.txt --location --request PUT \u0026quot;https://$controller_ip/api/v1/services/environments/prod/apps/arcadia-api/published-apis/arcadia-api-pub\u0026quot; --header 'Content-Type: application/json' --header 'Content-Type: text/plain' --data \u0026quot;@files/7controller/arcadia_api_pub.json\u0026quot;\rcurl -k -sb cookie.txt -c cookie.txt --location --request PUT \u0026quot;https://$controller_ip/api/v1/services/environments/prod/apps/arcadia-api/components/arcadia-stocks\u0026quot; --header 'Content-Type: application/json' --header 'Content-Type: text/plain' --data \u0026quot;@files/7controller/arcadia_stocks_component.json\u0026quot;\rWe have just uploaded the OpenApi spec to the Nginx Controller.\n"
},
{
	"uri": "/040_ingress/",
	"title": "Increase availability, security and application performance with Kubernetes Nginx Ingress",
	"tags": [],
	"description": "",
	"content": "Previously we have deployed the application but did not expose the services.\nWe need to be able to route the requests to the relevant service.\nNginx Kubernetes Ingress to save the day! :) The NGINX Ingress Controller for Kubernetes provides enterprise‑grade delivery services for Kubernetes applications, with benefits for users of both NGINX Open Source and NGINX Plus. With the NGINX Ingress Controller for Kubernetes, you get basic load balancing, SSL/TLS termination, support for URI rewrites, and upstream SSL/TLS encryption. NGINX Plus users additionally get session persistence for stateful applications and JSON Web Token (JWT) authentication for APIs.\n"
},
{
	"uri": "/050_service_mesh/040_opentracing/",
	"title": "Service Mesh Visibility",
	"tags": [],
	"description": "",
	"content": "In order to be able and trace and monitor all of our application calls we will need to expose the Grafana and Zipkin servers\n Expose Grapha and Zipkin.\nYou can ignore any warning messages generated by the bellow command.  kubectl get svc -n nginx-mesh grafana -oyaml | sed 's/ClusterIP/LoadBalancer/g' | kubectl apply -f -\rkubectl get svc -n nginx-mesh zipkin -oyaml | sed 's/ClusterIP/LoadBalancer/g' | kubectl apply -f -\rGet the svc external ip  kubectl get svc grafana zipkin -n nginx-mesh\rOutput\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\rgrafana LoadBalancer 172.20.176.7 a85065a76e142478fb29cdd2f2904e82-320541984.eu-central-1.elb.amazonaws.com 3000:31309/TCP 4d23h\rzipkin LoadBalancer 172.20.88.44 a7c02ef8a39b74da4ad41085261859f8-765188493.eu-central-1.elb.amazonaws.com 9411:31661/TCP 4d23h\r Browse to the Grafana dashboard http://\u0026lt;GRAFANA-EXTERNAL-IP\u0026gt;:3000  You can get a general look for you application status. The Grafana dashboards can be customized to your hearts desire.\n Go and buy or sell crypto coin. We will use Zipkin to trace this operation through the different services.   Browse to the Zipkin dashboard http://\u0026lt;ZIPKIN-EXTERNAL-IP\u0026gt;:9411 Create a filter as in the bellow image. Click on the result that looks like the first entry.   When buy or selling crypto the HTTP from the browser is sent to the application.\n1 - First it gets to the Nginx Ingress\n2 - The ingress forwards it to the stock transaction service\n3 - In order to perform the operation the stock transaction service contacts the user service to get the current account id\n4 and 5 - Next will again contact the users service to get the account information ( balance and crypto currencies), the stocks service to get the current buy and sell prices.\nOnce the operation has been validated and approved it will be recorded in the database and a success response returned to the client.\n  "
},
{
	"uri": "/070_controller/050_explore/",
	"title": "Explore the configuration",
	"tags": [],
	"description": "",
	"content": "We have automatically done all our configuration. Let explore it.\n Navigate to the imported configuration:  \u0026ldquo;N\u0026rdquo; -\u0026gt; \u0026ldquo;Services\u0026rdquo; -\u0026gt; \u0026ldquo;APIs\u0026rdquo; -\u0026gt; Click \u0026ldquo;arcadia-api\u0026rdquo; -\u0026gt; Click the edit button Inspect the published APIs  Click \u0026ldquo;Routing\u0026rdquo; You can observe the \u0026ldquo;arcadia-stocks\u0026rdquo; component. This is the only API we have published and is in charge of responding with current crypto currencies prices.\nThis API is also accessed by the browser application and is not limited.\nWe have published this API for thinrd party consumers and we have added authentication and rate limiting.\n"
},
{
	"uri": "/050_service_mesh/050_mesh_acl/",
	"title": "Mesh Access List",
	"tags": [],
	"description": "",
	"content": "The Kubernetes networking environment by default is an open network without restrictions or limitations.\nThis means that any pod can access any other pod or service. This introduces potential risks.\nOur arcadia-users exposes an internal API /v1/user_i/ which is used by the arcadia-login service only. This means that no other service should have access to it. Lets see what security implications this has:\n Access the arcadia-stocks pod. This pod is used only to get current crypto prices and nothing else. It should not be able to contact any other pod.  export arcadia_stocks_pod=$(kubectl get pods --selector=app=arcadia-stocks | grep arcadia-stocks -m 1 | cut -d' ' -f1)\rkubectl exec -it $arcadia_stocks_pod -- bash\rOutput\nDefaulting container name to arcadia-stocks.\rUse \u0026#39;kubectl describe pod/arcadia-stocks-7bd6ff78c8-pb6bn -n default\u0026#39; to see all of the containers in this pod.\rroot@arcadia-stocks-7bd6ff78c8-pb6bn:/usr/src/app#\r You are now in the arcadia-stocks container, the bellow command will access the arcadia-users internal API and get specific user information.\nThis should not be allowed. This call must be allowed only from the arcadia-login pods.  curl http://arcadia-users/v1/user_i/c29yaW5AbmdpbnguY29t\rOutput\n{\u0026#34;_id\u0026#34;:\u0026#34;6072b09ce6915d87d36e66af\u0026#34;,\u0026#34;accountId\u0026#34;:\u0026#34;47808892\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Sorin Boiangiu\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;sorin@nginx.com\u0026#34;,\u0026#34;cash\u0026#34;:121973.826,\u0026#34;password\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;stocks\u0026#34;:{\u0026#34;btc\u0026#34;:1.1989999999999994,\u0026#34;eth\u0026#34;:3.5,\u0026#34;ltc\u0026#34;:40.1},\u0026#34;picture\u0026#34;:\u0026#34;default\u0026#34;}\r exit the container bash.\nImprove you application security by introducing these checks and allowing access only from the arcadia-login pods  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: arcadia-users-sa\r---\rapiVersion: v1\rkind: ServiceAccount\rmetadata:\rname: arcadia-login-sa\r---\rapiVersion: specs.smi-spec.io/v1alpha3\rkind: HTTPRouteGroup\rmetadata:\rname: route-group\rspec:\rmatches:\r- name: destination-traffic\rmethods:\r- GET\r---\rapiVersion: access.smi-spec.io/v1alpha2\rkind: TrafficTarget\rmetadata:\rname: traffic-target\rspec:\rdestination:\rkind: ServiceAccount\rname: arcadia-users-sa\rrules:\r- kind: HTTPRouteGroup\rname: route-group\rmatches:\r- destination-traffic\rsources:\r- kind: ServiceAccount\rname: arcadia-login-sa\rEOF\rAttach the service account to the pods  kubectl set serviceaccount deployments/arcadia-users arcadia-users-sa\rkubectl set serviceaccount deployments/arcadia-login arcadia-login-sa\rAccess the arcadia-stocks pod and retry getting user information from the arcadia-users service, this time it will be blocked.  export arcadia_stocks_pod=$(kubectl get pods --selector=app=arcadia-stocks | grep arcadia-stocks -m 1 | cut -d' ' -f1)\rkubectl exec -it $arcadia_stocks_pod -- bash\rcurl http://arcadia-users/v1/user_i/c29yaW5AbmdpbnguY29t\rOutput\n\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;403 Forbidden\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;403 Forbidden\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt;\r\u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx/1.19.5\u0026lt;/center\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r exit the container bash.\nAccess the arcadia-login pod and try getting user information from the arcadia-users service, only this service should be allowed.  export arcadia_login_pod=$(kubectl get pods --selector=app=arcadia-login | grep arcadia-login -m 1 | cut -d' ' -f1)\rkubectl exec -it $arcadia_login_pod -- bash\rcurl http://arcadia-users/v1/user_i/c29yaW5AbmdpbnguY29t\rOutput\n{\u0026#34;_id\u0026#34;:\u0026#34;6072b09ce6915d87d36e66af\u0026#34;,\u0026#34;accountId\u0026#34;:\u0026#34;47808892\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Sorin Boiangiu\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;sorin@nginx.com\u0026#34;,\u0026#34;cash\u0026#34;:121973.826,\u0026#34;password\u0026#34;:\u0026#34;nginx\u0026#34;,\u0026#34;stocks\u0026#34;:{\u0026#34;btc\u0026#34;:1.1989999999999994,\u0026#34;eth\u0026#34;:3.5,\u0026#34;ltc\u0026#34;:40.1},\u0026#34;picture\u0026#34;:\u0026#34;default\u0026#34;}\r exit the container bash.\n"
},
{
	"uri": "/040_ingress/060_mtls_1/",
	"title": "Mutual TLS",
	"tags": [],
	"description": "",
	"content": "Enabling MTLS on our Nginx Ingress Controller is quite simple.\n Create the Certificate Secret and a policy to use on the Virtual Server resource  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rkind: Secret\rmetadata:\rname: ingress-mtls-secret\rapiVersion: v1\rtype: nginx.org/ca\rdata:\rca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQvVENDQXVXZ0F3SUJBZ0lVSzdhbU14OFlLWG1BVG51SkZETDlWS2ZUR2ZNd0RRWUpLb1pJaHZjTkFRRUwKQlFBd2dZMHhDekFKQmdOVkJBWVRBbFZUTVFzd0NRWURWUVFJREFKRFFURVdNQlFHQTFVRUJ3d05VMkZ1SUVaeQpZVzVqYVhOamJ6RU9NQXdHQTFVRUNnd0ZUa2RKVGxneEREQUtCZ05WQkFzTUEwdEpRekVXTUJRR0ExVUVBd3dOCmEybGpMbTVuYVc1NExtTnZiVEVqTUNFR0NTcUdTSWIzRFFFSkFSWVVhM1ZpWlhKdVpYUmxjMEJ1WjJsdWVDNWoKYjIwd0hoY05NakF3T1RFNE1qQXlOVEkyV2hjTk16QXdPVEUyTWpBeU5USTJXakNCalRFTE1Ba0dBMVVFQmhNQwpWVk14Q3pBSkJnTlZCQWdNQWtOQk1SWXdGQVlEVlFRSERBMVRZVzRnUm5KaGJtTnBjMk52TVE0d0RBWURWUVFLCkRBVk9SMGxPV0RFTU1Bb0dBMVVFQ3d3RFMwbERNUll3RkFZRFZRUUREQTFyYVdNdWJtZHBibmd1WTI5dE1TTXcKSVFZSktvWklodmNOQVFrQkZoUnJkV0psY201bGRHVnpRRzVuYVc1NExtTnZiVENDQVNJd0RRWUpLb1pJaHZjTgpBUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTmFINVRzaTZzaUFsU085dEJnYmY3VVRwcWowMUhRTlQ2UjhtQy9pCjhLYXFaSW9XSUdvN2xhTW9xTDYydTc4ay9WOHM2Z0FJaU1DSzBjekFvTFhNSnlJQkxQeTg4Yzdtc2xwZXgxTkEKVmRtMkVTVkN6bVlERE1TT3FpVmszWmpYeC9URmo2QzhNRFhhRkZUWFg1dWdtbWdscnFCWlh0OVI5VVBwVTJMNwo1bEZ0NlJ2R3VGczgvbVZORVR5c1A0SFhCWlh2ZE9mdG1YWUkvK01hOW5CMzIzNjdmcTI0L0RKZ2YvK2xRbUsxCkJLR3poSTZSc1pSSmdWOXdpK1VuZTBYNjlaS2lLOFdXU3lZS252YnRrcHZuTDA2dGNJaXJZNi80UzZ4Sm1HRVQKZEJUNmVxc0NoSUpQUStWSEp5dTROdnV6WmVCUXpGdmMwNytnUGZkVWZra1FXODhDQXdFQUFhTlRNRkV3SFFZRApWUjBPQkJZRUZKUGdhcnFYa00rdEJ0djVhdndTUWhUQmpTU2VNQjhHQTFVZEl3UVlNQmFBRkpQZ2FycVhrTSt0CkJ0djVhdndTUWhUQmpTU2VNQThHQTFVZEV3RUIvd1FGTUFNQkFmOHdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUIKQUl3WXpoY0s4OWtRL0xGWjZFRHgrQWp2bnJTVSs1cmdwQkgrRjVTNUUyY3pXOE5rNXhySnl0Y0ZUbUtlKzZScwpENHlxeTZSVVFEeWNYaDlPelBjbzgzYTBoeFlCZ1M5MWtJa25wYWF4dndLRDJleWc3UGNnK1lkS1FhZFlMcUY0CmI3cWVtc1FVVkpOWHdkZS9VanRBejlEOTh4dngwM2hQY2Qwb2dzUUhWZ21BZVpFd2l3UzFmTy9WNUE4dTl3MEkKcHlJRTVReXlHcHNpS2dpalpiMmhrS05RVHVJcEhiVnFydVA4eEV6TlFnamhkdS9uUW5OYy9lRUltVUlrQkFUVQpiSHdQc2xwYzVhdVV1TXJxR3lEQ0p2QUJpV3J2SmE3Yi9XcmtDT3FUWVhtR2NGM0w1ZU9FeTBhYkp0M2NNcSs5CnJLTUNVQWlkNG0yNEthWnc3OUk2anNBPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\r---\rapiVersion: k8s.nginx.org/v1\rkind: Policy\rmetadata:\rname: ingress-mtls-policy\rspec:\ringressMTLS:\rclientCertSecret: ingress-mtls-secret\rverifyClient: \u0026quot;on\u0026quot;\rverifyDepth: 1\rEOF\rAttach the MTLS policy to the VS.  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: k8s.nginx.org/v1\rkind: VirtualServer\rmetadata:\rname: arcadia\rspec:\rhost: $nginx_ingress tls:\rsecret: arcadia-wildcard # Represents the server certificate\rredirect:\renable: true # Always redirect to https if incoming request is http\rpolicies:\r- name: ingress-mtls-policy\rupstreams:\r- name: arcadia-users\rservice: arcadia-users\rport: 80\rhealthCheck: # This is the most basic healthcheck config for more info follow this link https://docs.nginx.com/nginx-ingress-controller/configuration/virtualserver-and-virtualserverroute-resources/#upstream-healthcheck\renable: true\rpath: /healthz\r- name: arcadia-login\rservice: arcadia-login\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stocks\rservice: arcadia-stocks\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stock-transaction\rservice: arcadia-stock-transaction\rport: 80\rhealthCheck: enable: true\rpath: /healthz\r- name: arcadia-frontend\rservice: arcadia-frontend\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\rroutes:\r- path: /v1/user # These directives attach the JWT policy to the route that needs authentication extract the username/email address and add it as a header policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-users\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: /v1/login action:\rpass: arcadia-login\r- path: /v1/stock action:\rpass: arcadia-stocks\r- path: /v1/stockt policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-stock-transaction\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: / action:\rpass: arcadia-frontend\rEOF\rVerify this is actually working by running the bellow command, it will use the client cert/key pair on the Cloud9 instance to authenticate:  curl -k https://$nginx_ingress/ --cert ./files/4ingress/client-cert.pem --key ./files/4ingress/client-key.pem\rWe are finished with this part of our experience .\nBefore moving forward reapply the ingress configuration without the mutual tls configuration  cat \u0026lt;\u0026lt; EOF | kubectl apply -f -\rapiVersion: k8s.nginx.org/v1\rkind: VirtualServer\rmetadata:\rname: arcadia\rspec:\rhost: $nginx_ingress tls:\rsecret: arcadia-wildcard # Represents the server certificate\rredirect:\renable: true # Always redirect to https if incoming request is http\rupstreams:\r- name: arcadia-users\rservice: arcadia-users\rport: 80\rhealthCheck: # This is the most basic healthcheck config for more info follow this link https://docs.nginx.com/nginx-ingress-controller/configuration/virtualserver-and-virtualserverroute-resources/#upstream-healthcheck\renable: true\rpath: /healthz\r- name: arcadia-login\rservice: arcadia-login\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stocks\rservice: arcadia-stocks\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\r- name: arcadia-stock-transaction\rservice: arcadia-stock-transaction\rport: 80\rhealthCheck: enable: true\rpath: /healthz\r- name: arcadia-frontend\rservice: arcadia-frontend\rport: 80\rhealthCheck:\renable: true\rpath: /healthz\rroutes:\r- path: /v1/user # These directives attach the JWT policy to the route that needs authentication extract the username/email address and add it as a header policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-users\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: /v1/login action:\rpass: arcadia-login\r- path: /v1/stock action:\rpass: arcadia-stocks\r- path: /v1/stockt policies:\r- name: jwt-policy\raction:\rproxy:\rupstream: arcadia-stock-transaction\rrequestHeaders:\rset:\r- name: okta-user\rvalue: \\${jwt_claim_email}\r- path: / action:\rpass: arcadia-frontend\rEOF\r"
},
{
	"uri": "/050_service_mesh/",
	"title": "Nginx Service Mesh",
	"tags": [],
	"description": "",
	"content": "We are pleased to introduce a development release of NGINX Service Mesh (NSM), a fully integrated lightweight service mesh that leverages a data plane powered by NGINX Plus to manage container traffic in Kubernetes environments.\nNSM secures applications in a zero‑trust environment by seamlessly applying encryption and authentication to container traffic. It delivers observability and insights into transactions to help organizations deploy and troubleshoot problems quickly and accurately. It also provides fine‑grained traffic control, enabling DevOps teams to deploy and optimize application components while empowering Dev teams to build and easily connect their distributed applications.\n"
},
{
	"uri": "/060_security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "The Nginx App Protect will allow to improve the application security posture, especially against OWASP Top 10 attacks.\nThe Nginx App Protect is deployed in the ingress resource therefor is able to see all traffic coming to our application from the outside world.\nWe\u0026rsquo;ll be able to bring security closer to the application and the development cycle and integrate it into CI/CD pipelines. This will allow to minimize false positives, since the web application firewall policy becomes a part of the application and is always tested as such.\n"
},
{
	"uri": "/070_controller/060_test/",
	"title": "Testing the APIs",
	"tags": [],
	"description": "",
	"content": "Our APIs are published, lets verify all works as expected.\n Check that the authentication is failing when not providing an API key  curl -k# https://$microhost/v1/stock/ticker/all\rOutput\n\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;401 Authorization Required\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;401 Authorization Required\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt;\r\u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Try again but this time including the API key and the test-client value  curl -k# https://$microhost/v1/stock/ticker/all --header \u0026quot;apikey: 0123456789\u0026quot;\rOutput\n{\u0026#34;btc\u0026#34;:{\u0026#34;ticker\u0026#34;:{\u0026#34;symbol\u0026#34;:\u0026#34;tBTCUSD\u0026#34;,\u0026#34;bid\u0026#34;:53783,\u0026#34;bidSize\u0026#34;:9.98263374,\u0026#34;ask\u0026#34;:53789,\u0026#34;askSize\u0026#34;:8.07079989,\u0026#34;dailyChange\u0026#34;:-1057,\u0026#34;dailyChangePerc\u0026#34;:-0.0193,\u0026#34;lastPrice\u0026#34;:53789,\u0026#34;volume\u0026#34;:6229.6409748,\u0026#34;high\u0026#34;:56432,\u0026#34;low\u0026#34;:53441}},\u0026#34;eth\u0026#34;:{\u0026#34;ticker\u0026#34;:{\u0026#34;symbol\u0026#34;:\u0026#34;tETHUSD\u0026#34;,\u0026#34;bid\u0026#34;:2767.1,\u0026#34;bidSize\u0026#34;:334.64948837000003,\u0026#34;ask\u0026#34;:2767.9,\u0026#34;askSize\u0026#34;:198.96622354000002,\u0026#34;dailyChange\u0026#34;:55.8,\u0026#34;dailyChangePerc\u0026#34;:0.0206,\u0026#34;lastPrice\u0026#34;:2767.3,\u0026#34;volume\u0026#34;:72050.81102921,\u0026#34;high\u0026#34;:2796.1,\u0026#34;low\u0026#34;:2661.5}},\u0026#34;ltc\u0026#34;:{\u0026#34;ticker\u0026#34;:{\u0026#34;symbol\u0026#34;:\u0026#34;tLTCUSD\u0026#34;,\u0026#34;bid\u0026#34;:256.64,\u0026#34;bidSize\u0026#34;:632.84153383,\u0026#34;ask\u0026#34;:256.82,\u0026#34;askSize\u0026#34;:1257.55131368,\u0026#34;dailyChange\u0026#34;:0.23,\u0026#34;dailyChangePerc\u0026#34;:0.0009,\u0026#34;lastPrice\u0026#34;:256.72,\u0026#34;volume\u0026#34;:36688.87741606,\u0026#34;high\u0026#34;:262.73,\u0026#34;low\u0026#34;:250.29}}}\r The configuration is rate limiting to 5 request per 1 minute sliding window. Repeat the above command 6 times. The last request will be blocked due to reaching the rate limit.  Output\n\u0026lt;head\u0026gt;\u0026lt;title\u0026gt;429 Too Many Requests\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;center\u0026gt;\u0026lt;h1\u0026gt;429 Too Many Requests\u0026lt;/h1\u0026gt;\u0026lt;/center\u0026gt;\r\u0026lt;hr\u0026gt;\u0026lt;center\u0026gt;nginx\u0026lt;/center\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r "
},
{
	"uri": "/070_controller/",
	"title": "Nginx Controller",
	"tags": [],
	"description": "",
	"content": "We have finished the first part of the publishing our application, now we want to publish our APIs to be used by third party organizations. We will acomplish this using two components:\n Nginx Controller which will be used as an API Management Nginx Container will be the API Microgateway which will reside within the Kubernetes environment  "
},
{
	"uri": "/080_cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "If you have run this workshop on your own environment you might need to delete the created resources. Please follow the steps in the next part.\n"
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]